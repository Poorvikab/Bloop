# Bloop - AI-Powered Educational Learning Platform

<div align="center">

**Transform any concept into comprehensive learning experiences with AI-powered videos, personalized roadmaps, and interactive games.**

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-green.svg)](https://fastapi.tiangolo.com/)
[![Manim](https://img.shields.io/badge/Manim-Community-orange.svg)](https://www.manim.community/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![FFmpeg](https://img.shields.io/badge/FFmpeg-Latest-green.svg)](https://ffmpeg.org/)

</div>

---

## ðŸŽ¯ Overview

Bloop revolutionizes educational content creation by generating **fully animated explainer videos** from simple text queries or documents. Unlike traditional educational AI systems that produce static text or slides, Bloop creates dynamic visual explanations with synchronized narration and optional photorealistic talking avatars - all generated on-demand without precomputed assets.

### âœ¨ Key Features

- ðŸŽ¨ **Programmatic Animation Generation** - Create stunning Manim animations automatically from educational concepts
- ðŸŽ™ï¸ **Scene-Aligned Narration** - Neural TTS with perfect temporal synchronization
- ðŸ¤– **Talking Avatar Integration** - Photorealistic avatars using SadTalker for human-like delivery
- ðŸ§  **LLM-Orchestrated Pipeline** - Intelligent scene planning and automated error repair
- ðŸŽ¯ **Fault-Tolerant Rendering** - Scene-isolated execution with graceful degradation
- ðŸ“š **Document-Based Learning** - Generate videos from PDFs, textbooks, or research papers
- ðŸ—ºï¸ **Personalized Roadmaps** - AI-generated learning paths tailored to your level
- ðŸŽ® **Interactive Learning Games** - Engaging educational games with voice interaction

## ðŸ—ï¸ Architecture

Bloop employs a sophisticated microservices architecture designed for real-time video generation:

```
Client Query
    â”‚
    â–¼
FastAPI Gateway (QA Service)
    â”‚
    â”œâ”€â–º LLM Answer Generation
    â”‚   â””â”€â–º Document Analysis & Understanding
    â”‚
    â””â”€â–º Manim Video Service
        â”‚
        â”œâ”€â–º Scene Planning (LLM â†’ JSON)
        â”‚   â””â”€â–º Structured Scene Graphs
        â”‚
        â”œâ”€â–º Scene-by-Scene Rendering
        â”‚   â”œâ”€â–º Independent Execution Context
        â”‚   â”œâ”€â–º Error Detection & Capture
        â”‚   â”œâ”€â–º LLM-Based Auto Repair
        â”‚   â””â”€â–º Scene Skipping (if unrecoverable)
        â”‚
        â”œâ”€â–º TTS Generation (Per Scene)
        â”‚   â””â”€â–º Neural Text-to-Speech
        â”‚
        â”œâ”€â–º FFmpeg Audio-Video Alignment
        â”‚   â”œâ”€â–º Temporal Synchronization
        â”‚   â””â”€â–º Duration Padding
        â”‚
        â””â”€â–º Scene Stitching
            â””â”€â–º Final Animation Output
                â”‚
                â””â”€â–º (Optional) SadTalker Avatar
                    â”œâ”€â–º Audio Extraction
                    â”œâ”€â–º Avatar Rendering
                    â””â”€â–º Side-by-Side Composition
```

## Quick Start

### Prerequisites

- Python 3.8+
- FFmpeg
- LaTeX (for mathematical expressions)
- GPU (optional, for faster avatar generation)
- CUDA (optional, for SadTalker acceleration)

### 1. Clone the Repository

```bash
git clone https://github.com/poorvikab/bloop.git
cd bloop
```

### 2. Backend Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install Manim
pip install manim

# Install additional dependencies
sudo apt-get install libcairo2-dev pkg-config python3-dev
sudo apt-get install ffmpeg

# Set up environment variables
cp .env.example .env
# Edit .env with your configuration
```

### 3. Configure Environment

```env
# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key
AZURE_OPENAI_ENDPOINT=your-azure-endpoint  # Optional

# TTS Configuration
TTS_PROVIDER=azure  # or 'elevenlabs', 'google'
AZURE_SPEECH_KEY=your-speech-key
AZURE_SPEECH_REGION=eastus

# SadTalker Configuration (Optional)
SADTALKER_MODEL_PATH=./models/sadtalker
ENABLE_AVATAR=true

# File Paths
UPLOAD_DIR=uploads
OUTPUT_DIR=outputs
TEMP_DIR=temp

# Performance
MAX_SCENE_RETRIES=1
SCENE_TIMEOUT=120
PARALLEL_SCENES=false
```

### 4. Start the Server

```bash
# Development mode
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Production mode
gunicorn main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker
```

### 5. Access the API

Visit `http://localhost:8000/docs` for interactive API documentation.

## Core Features

### ASK MODE - Intelligent Learning Assistant

Bloop's Ask mode combines all the capabilities of NotebookLM with powerful visual content generation:

#### Video Generation
Bloop generates educational videos in real-time through a sophisticated pipeline:

**Scene Planning via LLMs**

```python
# Example scene structure
{
  "scenes": [
    {
      "id": "scene_1",
      "concept": "Pythagorean Theorem",
      "visual_intent": "Show right triangle with labeled sides",
      "duration": 8,
      "manim_hints": ["Polygon", "MathTex", "Animation"]
    }
  ]
}
```

**Automated Manim Code Generation**
- LLM generates Manim Python code for each scene
- Validates syntax and API compatibility
- Optimizes for rendering performance

**Scene-Isolated Rendering**

Each scene renders independently to prevent cascading failures:

```
Scene 1: âœ“ Success
Scene 2: âœ— Failed â†’ LLM Repair â†’ âœ“ Success
Scene 3: âœ“ Success
Scene 4: âœ— Failed â†’ LLM Repair â†’ âœ— Failed â†’ Skip
Scene 5: âœ“ Success
```

**Intelligent Error Repair**

When a scene fails:

1. The Manim error log is captured
2. The failing scene code + error is sent back to the LLM
3. A corrected scene is regenerated
4. Rendering is retried once
5. Scene is skipped if still invalid

This design ensures **progressive output generation** under uncertainty.

---

## **6. Audio Generation and Synchronization**

Narration is generated **per scene** using neural TTS.
FFmpeg is used to:

* Pad video if audio is longer
* Pad audio if video is longer
* Preserve temporal alignment without re-encoding loss

This guarantees narration always matches visual pacing.

---

## **7. Talking Avatar Integration**

Bloop optionally generates a **talking avatar** using SadTalker:

* Final video audio is extracted
* Audio + avatar image sent to SadTalker
* Avatar video is rendered asynchronously
* Final output combines:

  * Left: Educational animation
  * Right: Talking avatar

This avoids visual occlusion and preserves instructional clarity.

---

## **8. Side-by-Side Video Composition**

Instead of picture-in-picture overlays, Bloop uses **horizontal concatenation**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Manim Animation      â”‚  Talking Avatar        â”‚
â”‚  (Concept Visuals)    â”‚  (Narration)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This layout maintains both:

* Visual explanation fidelity
* Human-like delivery presence

---

## **9. Fault Tolerance & Reliability**

Bloop is designed with **failure as a first-class consideration**:

* Scene-level isolation
* Partial video delivery
* Graceful degradation
* No pipeline-wide crashes
* Deterministic outputs under retries

---

## **10. Implementation Stack**
```

| Component        | Technology         |
| ---------------- | ------------------ |
| Backend API      | FastAPI            |
| LLM              | GPT / Azure OpenAI |
| Animation        | Manim Community    |
| TTS              | Neural TTS         |
| Video Processing | FFmpeg             |
| Avatar           | SadTalker          |
| Orchestration    | Python subprocess  |
| Rendering        | On-demand          |
```
---


## **11. Conclusion**

Bloop demonstrates that **real-time, fully visual educational video generation** is feasible using current generative AI systems when combined with disciplined system design.

By treating visuals, audio, and avatars as first-class outputsâ€”and by embracing fault toleranceâ€”Bloop moves beyond text-based education toward **explainable, visual intelligence**.

---

# ðŸŽžï¸ Demo & GIF Previews

> Replace these placeholders with actual GIFs once uploaded.

### ðŸ§® Manim Animation Generation

<video src="https://github.com/poorvikab/bloop/raw/main/assets/gifs/manim_generation.mp4" controls width="100%"></video>



### ðŸ¤– Talking Avatar (SadTalker)

<video src="https://github.com/poorvikab/bloop/raw/main/assets/gifs/avatar_generation.mp4" controls width="100%"></video>

### ðŸ§© Final Side-by-Side Video

<video src="https://github.com/poorvikab/bloop/raw/main/assets/gifs/final_video.mp4" controls width="100%"></video>

---
